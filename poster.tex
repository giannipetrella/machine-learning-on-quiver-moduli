\documentclass[final,20pt]{beamer}
\input{poster-preamble.tex}

\newcommand{\todo}[1]{{\color{blue}#1}}
% ====================
% Packages
% ====================

% \usepackage[T1]{fontenc}
% \usepackage{lmodern}
\usepackage[size=custom, width=76.2, height=101.6, scale=1.0]{beamerposter}
\usetheme{gemini}
\usecolortheme{gemini}
% \usepackage{graphicx}
\usepackage{epstopdf}  % Include the epstopdf package
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usepackage{anyfontsize}
\usepackage{booktabs}


\usepackage[backend=biber, datamodel=mrnumber, maxbibnames=99, sortcites]{biblatex}
\addbibresource{poster.bib}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.3\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% ====================
% Title
% ====================

\title{Machine learning features of quiver moduli}

\author{Gianni Petrella \inst{1}}

\institute[shortinst]{\inst{1} University of Luxembourg}
% ====================
% Footer (optional)
% ====================

\footercontent{
  \href{https://giannipetrella.eu}{www.giannipetrella.eu} \hfill
  Princeton ML Theory Summer School - August 12-21, 2025}
  % Computations in Algebra and Geometry, ETH - August 25-29, 2025}

% ====================
% Logo (optional)
% ====================

% Include Eurecom logo on the right side of the header
\logoright{\includegraphics[height=7cm]{UNI-Logo-en-rgb.png}}
% Include Lab logo on the left side of the header
% \logoleft{\includegraphics[height=7cm]{./logos/s3logo.png}}

% ====================
% Body
% ====================

\begin{document}

\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Representations of quivers}

  A \emph{quiver} $Q$ is a finite directed multigraph with vertices $Q_0$ and arrows $Q_1$.
  A \emph{representation} $V$ of $Q$ is a choice of vector space $V_i$ per vertex $i$
  and a choice of linear application $M_{\alpha}$ for each arrow $\alpha$ \cite{2311.17003}.

  The \emph{dimension vector} of a representation $V$
  is~$\tuple{d} \colonequals \dim(V) \colonequals (\dim(V_i))_{i \in Q_0}$.

    \begin{figure}
      \centering
      \begin{tikzpicture}
        \node (1) at (0, 0)              {$\bullet$};
        \node (4) [above of = 1]              {$1$};
        \node (2) at (6, 0)              {$\bullet$};
        \node (5) [above of = 2]               {$3$};
        \node (3) at (6, -6)             {$\bullet$};
        \node (6) [above of = 3]              {$5$};

        \draw[->, bend left  = 30] (1) edge (2);
        \draw[->, bend left  = 10] (1) edge (2);
        \draw[->, bend right = 10] (1) edge (2);
        \draw[->, bend right = 30] (1) edge (2);
        \draw[->, bend left  = 10] (1) edge (3);
        \draw[->, bend right = 10] (1) edge (3);
        \draw[->]                  (2) edge (3);
      \end{tikzpicture}
      \caption{A quiver with three vertices.}
    \end{figure}

    A representation is, equivalently, a point
    in the \emph{representation space}
    \begin{equation}
      \repspace(Q, \tuple{d}) \colonequals \bigoplus_{\alpha \in Q_1} \operatorname{Mat}_{d_{t\alpha, s\alpha}}(\CC)
    \end{equation}

    Two representations are \emph{isomorphic} if they are equivalent up to
    a change of basis -
    that is, if they lie in the same orbit of the action
    of $\GLd$ on $\repspace(Q, d)$.

    If one fixes a \emph{stability parameter} $\theta \in \ZZ^{Q_0}$ for which
    $\theta \cdot \tuple{d} = 0$, we say that a representation $V$
    is $\theta$-stable, respectively $\theta$-semistable,
    if all of its subrepresentations $W$ satisfy
    $\theta \cdot \dim(W) < 0$, respectively
    $\theta\cdot\dim(W)\leq 0$.

    \todo{why are quiver moduli useful}
    \end{block}

  \begin{block}{Moduli spaces of stable representations}

    The sets of $\theta$-stable and semistable representations
    are GIT-stable opens in $\repspace(Q, \tuple{d})$.
    We can thus consider their GIT quotients.

    The relations between the affine, semistable and stable quotients are summarised in the following diagram:
    \begin{equation}
      \begin{tikzcd}[row sep=small, column sep=small, ampersand replacement=\&]
        \repspace^{\theta\stable}(Q, \tuple{d}) \arrow[r,hook] \arrow[d]     \& \repspace^{\theta\semistable}(Q, \tuple{d}) \arrow[r,hook] \arrow[d]     \& \repspace(Q, \tuple{d}) \arrow[d] \\
        \repspace^{\theta\stable}(Q, \tuple{d})\gitquot\GLd \arrow[r, hook] \arrow[d, equal] \& \repspace^{\theta\semistable}(Q, \tuple{d})\gitquot\GLd \arrow[r, two heads] \arrow[d, equal] \& \repspace(Q, \tuple{d}) / \GLd \arrow[d, equal] \\
        \modulispace^{\theta\stable}(Q, \tuple{d}) \arrow[r, hook]          \& \modulispace^{\theta\semistable}(Q, \tuple{d}) \arrow[r, two heads]                \& \modulispace^{\semisimple}(Q, \tuple{d}).
      \end{tikzcd}
    \end{equation}

    It is known that horizontal inclusions are open and surjections are projective.
    Moreover, the moduli of stable representations is smooth, and
    $\mathcal{O}(\modulispace^{\semisimple}(Q, d))$ is generated by traces of products over
    oriented cycles in $Q$ \cite{MR958897}.
    If $Q$ is acyclic, $\modulispace^{\semisimple}(Q, d)$ is thus a point
    and $\modulispace^{\theta\semistable}(Q, d)$ is a projective variety.
    \vfill
  \end{block}

  \begin{alertblock}{Geometric invariants}

    Many geometric invariants of quiver moduli can be computed effectively
    using the software package \quivertools~\cite{quivertools,2506.19432}.
    However, complexity is often a limiting factor,
    so for large scale applications we attempt to model
    several of these features using machine learning techniques.

    \begin{itemize}
      \item \textbf{Euler characteristic} is obtained via Betti numbers computations \cite{MR1974891}.
      These are strong topological invariants of complex varieties.

      \item \textbf{(Strong) ample stability} are properties verified by enumerating
      all possible Harder--Narasimhan types and destabilizing subdimension vectors as in \cite{MR1974891,2311.17003}.
      These inform mathematicians on the ``behaviour'' of unstable representations
      of $Q$.

      \item \textbf{Teleman inequality bounds} are computed again
      for each Harder--Narasimhan type using \cite{2311.17003}.
      Teleman quantization is a key tool to infer higher cohomology vanishings
      on quiver moduli.
    \end{itemize}

  \end{alertblock}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}



  \begin{block}{Datasets}
    The training data is obtained by classical algorithms derived from
    the literature of quiver representations. These are implemented in
    \quivertools, see \cite{quivertools} and the accompanying \cite{2506.19432}.
    We harvested data by running the Julia version of \quivertools~on a cluster,
    using {\tt{Distributed.jl}} to parallelize the computations on a 128 core-CPU node,
    for a CPU time of ~1000h.
    We store this dataset in a SQLite database, which exists
    as part of unrelated work, and retrieve 50000 cases from it
    for training.
    In practice, we run tests on 

  \end{block}

  \begin{block}{Methodology}

    We train feedforward neural networks to predict various numerical values
    associated to a quiver and a dimension vector.
    For all of our prediction problems we use multi-layer perceptron architectures.
    The input of our network is always the adjacency matrix of the quiver, flattened,
    joined to the dimension vector.
    The dataset is split into training (80\%) and testing data (20\%).

    We use {\tt{Flux.jl}} \cite{}, a pure Julia ML stack.

   \begin{figure}
      \centering
      \begin{tikzpicture}[
      neuron/.style={circle, fill=black!25, minimum size=30pt, inner sep=0pt},
          input neuron/.style={neuron, fill=green!50},
          output neuron/.style={neuron, fill=red!50},
          hidden neuron/.style={neuron, fill=blue!50},
      ]
        \node[input neuron] (1-0) at (0,0) {};
        \node[input neuron] (1-1) at (0,2) {};
        \node             (1-mid) at (0,4) {\vdots};
        \node[input neuron] (1-2) at (0,6) {};
        \node[input neuron] (1-3) at (0,8) {};

        \node[hidden neuron] (2-0) at (3,-1) {};
        \node[hidden neuron] (2-1) at (3, 1) {};
        \node[hidden neuron] (2-2) at (3, 3) {};
        \node              (2-mid) at (3, 4) {\vdots};
        \node[hidden neuron] (2-3) at (3, 5) {};
        \node[hidden neuron] (2-4) at (3, 7) {};
        \node[hidden neuron] (2-5) at (3, 9) {};

        \foreach \source in {0,...,3}
          \foreach \target in {0,...,5}
            \draw[->] (1-\source) edge (2-\target);

        \node (3-0) at (6,-1) {\dots};
        \node (3-1) at (6, 1) {\dots};
        \node (3-2) at (6, 3) {\dots};
        \node (3-3) at (6, 5) {\dots};
        \node (3-4) at (6, 7) {\dots};
        \node (3-5) at (6, 9) {\dots};

        \foreach \source in {0,...,5}
          \foreach \target in {0,...,5}
            \draw[->] (2-\source) edge (3-\target);

        \node[hidden neuron] (4-0) at (9,-1) {};
        \node[hidden neuron] (4-1) at (9, 1) {};
        \node[hidden neuron] (4-2) at (9, 3) {};
        \node              (4-mid) at (9, 4) {\vdots};
        \node[hidden neuron] (4-3) at (9, 5) {};
        \node[hidden neuron] (4-4) at (9, 7) {};
        \node[hidden neuron] (4-5) at (9, 9) {};

        \foreach \source in {0,...,5}
          \foreach \target in {0,...,5}
            \draw[->] (3-\source) edge (4-\target);


        \node[hidden neuron] (5-0) at (12,-1) {};
        \node[hidden neuron] (5-1) at (12, 1) {};
        \node[hidden neuron] (5-2) at (12, 3) {};
        \node              (5-mid) at (12, 4) {\vdots};
        \node[hidden neuron] (5-3) at (12, 5) {};
        \node[hidden neuron] (5-4) at (12, 7) {};
        \node[hidden neuron] (5-5) at (12, 9) {};

        \foreach \source in {0,...,5}
          \foreach \target in {0,...,5}
            \draw[->] (4-\source) edge (5-\target);

        \node[output neuron] (6) at (15, 4) {};

        \foreach \source in {0,...,5}
          \draw[->] (5-\source) edge (6);

        \end{tikzpicture}
      \caption{A standard feedforward neural network.}
    \end{figure}

    Current results are based on the following choices of hyperparameters.
    We implemented a learning rate decay scheme that halves $\eta$
    whenever it detects a plateau in the training loss.

    \begin{table}
      \centering
      \begin{tabular}{lcc}
        \toprule
        Hyperparameter & Betti number & Teleman ratio\\
        \midrule
        Hidden layers                     & 3                 & 3\\
        Layer sizes                       & (4096, 4096, 512) & (4096, 4096, 512)\\
        Activations              & {\tt{relu}}       & {\tt{relu}}, sigmoid\\
        Learning rate             & $3 \cdot 10^{-4}$ & $5 \cdot 10^{-5}$\\
        Optimiser                         & Adam              & Adam\\
        \bottomrule
      \end{tabular}
      \caption{Hyperparameters used in current setup.}
    \end{table}


  \end{block}

  \begin{block}{Nam Cursus Consequat Egestas}

    Nulla eget sem quam. Ut aliquam volutpat nisi vestibulum convallis. Nunc a
    lectus et eros facilisis hendrerit eu non urna. Interdum et malesuada fames
    ac ante \textit{ipsum primis} in faucibus. Etiam sit amet velit eget sem
    euismod tristique. Praesent enim erat, porta vel mattis sed, pharetra sed
    ipsum. Morbi commodo condimentum massa, \textit{tempus venenatis} massa
    hendrerit quis. Maecenas sed porta est. Praesent mollis interdum lectus,
    sit amet sollicitudin risus tincidunt non.

    Etiam sit amet tempus lorem, aliquet condimentum velit. Donec et nibh
    consequat, sagittis ex eget, dictum orci. Etiam quis semper ante. Ut eu
    mauris purus. Proin nec consectetur ligula. Mauris pretium molestie
    ullamcorper. Integer nisi neque, aliquet et odio non, sagittis porta justo.

    \begin{itemize}
      \item \textbf{Sed consequat} id ante vel efficitur. Praesent congue massa
        sed est scelerisque, elementum mollis augue iaculis.
        \begin{itemize}
          \item In sed est finibus, vulputate nunc gravida, pulvinar lorem. In maximus nunc dolor, sed auctor eros
            porttitor quis.
          \item Fusce ornare dignissim nisi. Nam sit amet risus vel lacus
            tempor tincidunt eu a arcu.
          \item Donec rhoncus vestibulum erat, quis aliquam leo
            gravida egestas.
        \end{itemize}
      \item \textbf{Sed luctus, elit sit amet} dictum maximus, diam dolor
        faucibus purus, sed lobortis justo erat id turpis.
      \item \textbf{Pellentesque facilisis dolor in leo} bibendum congue.
        Maecenas congue finibus justo, vitae eleifend urna facilisis at.
    \end{itemize}

  \end{block}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \begin{exampleblock}{A Highlighted Block Containing Some Math}

    A different kind of highlighted block.

    $$
    \int_{-\infty}^{\infty} e^{-x^2}\,dx = \sqrt{\pi}
    $$

    Interdum et malesuada fames $\{1, 4, 9, \ldots\}$ ac ante ipsum primis in
    faucibus. Cras eleifend dolor eu nulla suscipit suscipit. Sed lobortis non
    felis id vulputate.

    \heading{A Heading Inside a Block}

    Praesent consectetur mi $x^2 + y^2$ metus, nec vestibulum justo viverra
    nec. Proin eget nulla pretium, egestas magna aliquam, mollis neque. Vivamus
    dictum $\mathbf{u}^\intercal\mathbf{v}$ sagittis odio, vel porta erat
    congue sed. Maecenas ut dolor quis arcu auctor porttitor.

    \heading{Another Heading Inside a Block}

    Sed augue erat, scelerisque a purus ultricies, placerat porttitor neque.
    Donec $P(y \mid x)$ fermentum consectetur $\nabla_x P(y \mid x)$ sapien
    sagittis egestas. Duis eget leo euismod nunc viverra imperdiet nec id
    justo.

  \end{exampleblock}

  \begin{block}{Outlook}

    PatternBoost is a ML-enabled mathematical research workflow
    presented in \cite{2411.00566}.
    It consists of two iterative phases:
    a classical search for mathematical objects
    and their properties conducted using classical algorithms,
    and a training phase
    in which the best results of the search
    are used to train a transformer neural network.
    Samples from the trained transformer are
    then used as starting point for the search phase again,
    and the process repeats.

    PatternBoost has been used to produce examples of mathematical objects
    that maximize certain quantities, namely exceeding conjectured bounds
    and thus disproving standing conjectures (see \cite[Section~3.3]{2411.00566}).

    \todo{add a scheme explaining PatternBoost}

    We apply the PatternBoost technique to search for quiver moduli
    that optimise currently open problems, e.g. in \cite{2311.17003}.
  \end{block}

  \begin{block}{References}

  \printbibliography

  \end{block}

\end{column}

\separatorcolumn
\end{columns}
\end{frame}

\end{document}
